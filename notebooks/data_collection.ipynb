{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fcd60e",
   "metadata": {},
   "source": [
    "##### Gridmet Data were most of the predictors presents more information about data is in Data composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gridmet Datasets combined after pulling from GEE(Google Earth Engine). More informations about datasets in Data composition. \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== paths (edit these) ====\n",
    "folder = Path('/Users/davidjerome/Desktop/Dissertation/XGBoost/Data collection/GRIDmeta data')\n",
    "gm_path   = folder / \"gridMET_daily_2024_ALL.csv\"\n",
    "ndvi_path = folder / \"NDVI_S2_10m_exactday_AVAILABLE_2024_ALL.csv\"\n",
    "out_path  = folder / \"gridMET_plus_NDVI_exact_2024_ALL.csv\"\n",
    "\n",
    "# ---- read both as strings for exact matching on keys ----\n",
    "# (this preserves the exact text for date/lat/lon; no float rounding)\n",
    "gm   = pd.read_csv(gm_path, dtype={\"date\": str, \"lat\": str, \"lon\": str})\n",
    "ndvi = pd.read_csv(ndvi_path, dtype={\"date\": str, \"lat\": str, \"lon\": str})\n",
    "\n",
    "# strip accidental whitespace on keys\n",
    "for df in (gm, ndvi):\n",
    "    df[\"date\"] = df[\"date\"].astype(str).str.strip()\n",
    "    df[\"lat\"]  = df[\"lat\"].astype(str).str.strip()\n",
    "    df[\"lon\"]  = df[\"lon\"].astype(str).str.strip()\n",
    "\n",
    "# keep only the NDVI column from the NDVI table for the join\n",
    "ndvi_small = ndvi[[\"date\", \"lat\", \"lon\", \"ndvi\"]]\n",
    "\n",
    "# left join: keep ALL gridMET rows; ndvi will be NaN when there is no exact match\n",
    "merged = gm.merge(ndvi_small, on=[\"date\", \"lat\", \"lon\"], how=\"left\")\n",
    "\n",
    "# save\n",
    "merged.to_csv(out_path, index=False)\n",
    "\n",
    "# quick stats\n",
    "matched = merged[\"ndvi\"].notna().sum()\n",
    "total   = len(merged)\n",
    "print(f\"Saved → {out_path}\")\n",
    "print(f\"Rows total: {total:,}\")\n",
    "print(f\"Rows with NDVI match: {matched:,} ({matched/total:.1%})\")\n",
    "print(f\"Rows without NDVI (left blank): {total-matched:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ndvi Combinied after pulling it from GEE(Google Earth Engine). More informations about datasets in Data composition.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# folder with your 12 monthly CSVs\n",
    "folder = Path('/Users/davidjerome/Desktop/Dissertation/XGBoost/Data collection/ndvi_final')   # <- change to your folder\n",
    "\n",
    "# pick files like: NDVI_S2_10m_exactday_AVAILABLE_2024_1.csv ... _12.csv\n",
    "files = list(folder.glob(\"NDVI_S2_10m_exactday_AVAILABLE_2024_*.csv\"))\n",
    "\n",
    "# sort by the month number at the end of the filename\n",
    "def month_key(p):\n",
    "    m = re.search(r\"_(\\d+)\\.csv$\", p.name)\n",
    "    return int(m.group(1)) if m else 0\n",
    "files = sorted(files, key=month_key)\n",
    "\n",
    "# keep column order from the first file; just append rows\n",
    "cols = pd.read_csv(files[0], nrows=0).columns\n",
    "dfs = [pd.read_csv(f, usecols=cols) for f in files]\n",
    "out = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "out_path = folder / \"NDVI_S2_10m_exactday_AVAILABLE_2024_ALL.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Combined:\", len(files), \"files →\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386126bb",
   "metadata": {},
   "source": [
    "##### Ndvi missing values were temporally interpolated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path('/Users/davidjerome/Desktop/Dissertation/XGBoost/Data collection/ndvi_final/ndvi_gaps_fill')\n",
    "df = pd.read_csv(p/\"gridMET_plus_NDVI_exact_2024_ALL.csv\")\n",
    "\n",
    "# Parse date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Sort properly\n",
    "df = df.sort_values([\"id\",\"lat\",\"lon\",\"date\"])\n",
    "\n",
    "# Replace 0 with NaN (zeros are usually cloud/no data)\n",
    "df[\"ndvi_clean\"] = df[\"ndvi\"].where(df[\"ndvi\"] > 0)\n",
    "\n",
    "# Interpolate NDVI per location\n",
    "def interp_group(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.set_index(\"date\").copy()\n",
    "    g[\"ndvi_interp\"] = g[\"ndvi_clean\"].interpolate(method=\"linear\", limit=30)  \n",
    "    # limit=30 means fill gaps ≤30 days; longer gaps stay NaN\n",
    "    return g.reset_index()\n",
    "\n",
    "df = df.groupby([\"id\",\"lat\",\"lon\"], group_keys=False).apply(interp_group)\n",
    "\n",
    "# Add missing flag\n",
    "df[\"ndvi_missing\"] = df[\"ndvi\"].isna().astype(int)\n",
    "\n",
    "out = p/\"gridMET_plus_NDVI_interp_2024.csv\"\n",
    "df.to_csv(out, index=False)\n",
    "print(\"Saved:\", out, \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missiing values from the previous dataset will be interpolated and showed as a graph in the next block of code\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------- settings (no paths) --------\n",
    "INFILE  = \"gridMET_plus_NDVI_exact_2024_ALL.csv\"\n",
    "OUTFILE = \"gridMET_plus_NDVI_interp_2024_ALL.csv\"\n",
    "TREAT_ZERO_AS_MISSING = True   # keep as True for clouds/non-veg zeros\n",
    "MAX_GAP_STEPS = 30             # linear interpolate across ≤ this many consecutive NaNs\n",
    "# ------------------------------------\n",
    "\n",
    "# 1) Read as TEXT to keep date/lat/lon formats EXACTLY as in the file\n",
    "df = pd.read_csv(INFILE, dtype=str)\n",
    "orig_index = df.index\n",
    "orig_cols  = df.columns.tolist()\n",
    "\n",
    "# 2) Helper columns ONLY for computation (originals stay untouched)\n",
    "#    Robust date parsing to datetime (we do NOT overwrite 'date')\n",
    "dtxt = df[\"date\"].astype(str).str.strip().str.split().str[0]\n",
    "ddt  = pd.to_datetime(dtxt, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "m = ddt.isna();  ddt.loc[m] = pd.to_datetime(dtxt[m], errors=\"coerce\", format=\"%m/%d/%Y\")\n",
    "m = ddt.isna();  ddt.loc[m] = pd.to_datetime(dtxt[m], errors=\"coerce\", format=\"%d/%m/%Y\")\n",
    "df[\"_date_dt\"] = ddt\n",
    "\n",
    "# numeric copy of NDVI for interpolation\n",
    "ndvi_num = pd.to_numeric(df[\"ndvi\"], errors=\"coerce\")\n",
    "if TREAT_ZERO_AS_MISSING:\n",
    "    ndvi_num = ndvi_num.where(ndvi_num != 0)\n",
    "df[\"_ndvi_num\"] = ndvi_num\n",
    "\n",
    "# 3) Linear TEMPORAL interpolation per (id,lat,lon) WITHOUT reindexing or touching date/lat/lon\n",
    "#    Fix for \"duplicate labels\": aggregate to unique dates first, then map back\n",
    "def interp_group(g: pd.DataFrame) -> pd.Series:\n",
    "    # average NDVI if multiple rows share the same date (rare but avoids duplicate-index issues)\n",
    "    per_day = g.groupby(\"_date_dt\")[\"_ndvi_num\"].mean().sort_index()\n",
    "    # time-based linear interpolation across up to MAX_GAP_STEPS consecutive NaNs\n",
    "    per_day_i = per_day.interpolate(method=\"time\", limit=MAX_GAP_STEPS)\n",
    "    # map the interpolated per-day series back to each original row's date, preserving order\n",
    "    return g[\"_date_dt\"].map(per_day_i)\n",
    "\n",
    "df[\"ndvi_interp\"] = (\n",
    "    df.groupby([\"id\",\"lat\",\"lon\"], group_keys=False)\n",
    "      .apply(interp_group)\n",
    ")\n",
    "\n",
    "# 4) Optional flag: original NDVI missing (kept for modeling/QA)\n",
    "df[\"ndvi_missing_flag\"] = df[\"ndvi\"].isna().astype(\"Int64\")\n",
    "\n",
    "# 5) Restore original row order and save with NEW columns appended\n",
    "df = df.loc[orig_index]\n",
    "out_cols = orig_cols + [c for c in [\"ndvi_interp\",\"ndvi_missing_flag\"] if c not in orig_cols]\n",
    "df[out_cols].to_csv(OUTFILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17cfbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This set code reperesents the line graph in the same duration that differentiates\n",
    "## the default data missing points vs how interpolated points filled that gap.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Parse dates\n",
    "df[\"_date_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Convert NDVI columns to numeric\n",
    "ndvi_raw = pd.to_numeric(df[\"ndvi\"], errors=\"coerce\")\n",
    "ndvi_interp = pd.to_numeric(df[\"ndvi_interp\"], errors=\"coerce\")\n",
    "\n",
    "# Monthly means\n",
    "monthly_raw = ndvi_raw.groupby(df[\"_date_dt\"].dt.month).mean()\n",
    "monthly_interp = ndvi_interp.groupby(df[\"_date_dt\"].dt.month).mean()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(monthly_raw.index, monthly_raw.values, marker=\"o\", linestyle=\"--\",\n",
    "         label=\"Original NDVI\", alpha=0.7)\n",
    "plt.plot(monthly_interp.index, monthly_interp.values, marker=\"o\", linestyle=\"-\",\n",
    "         label=\"Interpolated NDVI\", alpha=0.9)\n",
    "\n",
    "plt.xticks(range(1,13), [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\n",
    "                         \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average NDVI\")\n",
    "plt.title(\"Monthly Average NDVI (Original vs Interpolated)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743856a5",
   "metadata": {},
   "source": [
    "### Topography and human proximities were directly pulled from GEE (Google Earth Engine). More information about the data is in Data composition. Lets look about fire data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b9cf14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5387b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc52bf35",
   "metadata": {},
   "source": [
    "##### Fire data were directly pulled from NASA Fire firms. More information about this dataset will be in data composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "\n",
    "FINAL = \"final_dataset_with_labels.csv\"  # after you've already merged MODIS labels (id,lat,lon,date,...,fire_modis)\n",
    "FIRMS = [\n",
    "    \"fire_archive_J1V-C2_661605.csv\",   # add all you downloaded\n",
    "    \"fire_archive_SV-C2_661607.csv\",\n",
    "    \"fire_nrt_J2V-C2_661606.csv\"\n",
    "]\n",
    "BUFFER_M   = 1500         # start generous; tighten later\n",
    "DAY_TOL    = 1            # UTC/local safety\n",
    "CONF_NUM   = 80           # set None to skip; or use text filter below\n",
    "\n",
    "def norm_date(s):\n",
    "    d = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True, utc=False)\n",
    "    m = d.isna()\n",
    "    if m.any(): d.loc[m] = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=True)\n",
    "    m = d.isna()\n",
    "    if m.any(): d.loc[m] = pd.to_datetime(s[m], errors=\"coerce\")\n",
    "    return d.dt.date\n",
    "\n",
    "# 1) Load final (points × days) with MODIS labels already inside\n",
    "final = pd.read_csv(FINAL)\n",
    "final[\"date\"] = norm_date(final[\"date\"])\n",
    "gpts = gpd.GeoDataFrame(\n",
    "    final[[\"id\",\"lat\",\"lon\",\"date\"]].drop_duplicates(),\n",
    "    geometry=[Point(xy) for xy in zip(final[\"lon\"], final[\"lat\"])],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "gpts_m = gpts.to_crs(3857)\n",
    "gpts_m[\"geometry\"] = gpts_m.buffer(BUFFER_M)\n",
    "\n",
    "# 2) Load/union FIRMS and standardize\n",
    "fires = []\n",
    "for f in FIRMS:\n",
    "    df = pd.read_csv(f)\n",
    "    # Required fields\n",
    "    keep = {\"latitude\",\"longitude\",\"acq_date\",\"confidence\",\"daynight\"}\n",
    "    missing = [c for c in [\"latitude\",\"longitude\",\"acq_date\"] if c not in df.columns]\n",
    "    if missing: raise ValueError(f\"{f} missing {missing}\")\n",
    "    df[\"date\"] = norm_date(df[\"acq_date\"])\n",
    "    # Confidence filter (numeric or text)\n",
    "    if \"confidence\" in df.columns:\n",
    "        try:\n",
    "            df = df[df[\"confidence\"].astype(float) >= CONF_NUM]\n",
    "        except:\n",
    "            df = df[df[\"confidence\"].astype(str).str.lower().isin({\"nominal\",\"high\"})]\n",
    "    fires.append(df[[\"latitude\",\"longitude\",\"date\"]])\n",
    "\n",
    "fires = pd.concat(fires, ignore_index=True)\n",
    "gfires = gpd.GeoDataFrame(\n",
    "    fires,\n",
    "    geometry=[Point(xy) for xy in zip(fires[\"longitude\"], fires[\"latitude\"])],\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(3857)\n",
    "\n",
    "# 3) Label by date (±DAY_TOL) + buffer\n",
    "labels = []\n",
    "for d in sorted(gpts_m[\"date\"].unique()):\n",
    "    mask = (gfires[\"date\"] >= (d - timedelta(days=DAY_TOL))) & (gfires[\"date\"] <= (d + timedelta(days=DAY_TOL)))\n",
    "    fires_d = gfires.loc[mask]\n",
    "    pbuf_d  = gpts_m[gpts_m[\"date\"] == d][[\"id\",\"date\",\"geometry\"]]\n",
    "\n",
    "    if fires_d.empty:\n",
    "        out = pbuf_d[[\"id\",\"date\"]].copy()\n",
    "        out[\"fire_firms\"] = 0\n",
    "        out[\"min_dist_m\"] = np.nan\n",
    "        labels.append(out)\n",
    "        continue\n",
    "\n",
    "    # Spatial join to mark hits\n",
    "    joined = gpd.sjoin(pbuf_d, fires_d[[\"geometry\"]], how=\"left\", predicate=\"intersects\")\n",
    "    hit_ids = set(joined.dropna(subset=[\"index_right\"])[\"id\"].values)\n",
    "\n",
    "    out = pbuf_d[[\"id\",\"date\"]].copy()\n",
    "    out[\"fire_firms\"] = out[\"id\"].isin(hit_ids).astype(int)\n",
    "\n",
    "    # Also compute nearest distance (diagnostic)\n",
    "    tree = KDTree(np.vstack([fires_d.geometry.x, fires_d.geometry.y]).T)\n",
    "    pts_xy = np.vstack([pbuf_d.geometry.centroid.x, pbuf_d.geometry.centroid.y]).T\n",
    "    dist, _ = tree.query(pts_xy, k=1)\n",
    "    out[\"min_dist_m\"] = dist[:,0]  # meters in EPSG:3857\n",
    "    labels.append(out)\n",
    "\n",
    "labels = pd.concat(labels, ignore_index=True)\n",
    "\n",
    "# 4) Merge back and combine with MODIS  (SAFE VERSION)\n",
    "\n",
    "# If we never created any labels (e.g., no fires matched any date), make an empty table\n",
    "if 'labels' not in locals() or labels is None or labels.empty:\n",
    "    import pandas as pd\n",
    "    labels = pd.DataFrame(columns=[\"id\",\"date\",\"fire_firms\"])\n",
    "\n",
    "# Ensure the expected columns exist\n",
    "if \"fire_firms\" not in labels.columns:\n",
    "    labels[\"fire_firms\"] = 0\n",
    "\n",
    "# Do the merge\n",
    "final = final.merge(labels[[\"id\",\"date\",\"fire_firms\"]], on=[\"id\",\"date\"], how=\"left\")\n",
    "\n",
    "# Guarantee the column exists after merge and fill NaNs with 0\n",
    "if \"fire_firms\" not in final.columns:\n",
    "    final[\"fire_firms\"] = 0\n",
    "else:\n",
    "    final[\"fire_firms\"] = final[\"fire_firms\"].fillna(0).astype(int)\n",
    "\n",
    "# If MODIS column is missing, create it as zeros\n",
    "if \"fire_modis\" not in final.columns:\n",
    "    final[\"fire_modis\"] = 0\n",
    "\n",
    "# Final combined label: 1 if either MODIS or FIRMS is 1\n",
    "final[\"fire_label\"] = ((final[\"fire_modis\"] == 1) | (final[\"fire_firms\"] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c38ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After run this code I came to find out that fire data (Target) is primary for\n",
    "## classification task has a huge imbalance and it definetly need a imbalance \n",
    "## method\n",
    "\n",
    "print(\"labels shape:\", labels.shape, \"columns:\", list(labels.columns))\n",
    "print(\"labels sample:\\n\", labels.head())\n",
    "\n",
    "print(\"final columns after merge:\", list(final.columns))\n",
    "print(\"fire_firms value counts:\", final.get(\"fire_firms\", 0).__class__.__name__,\n",
    "      \"\\n\", final[\"fire_firms\"].value_counts(dropna=False) if \"fire_firms\" in final.columns else \"no column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8113f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8164a262",
   "metadata": {},
   "source": [
    "### Final merging of all different datasets into a one final dataset to push into a model and to achieve our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6f4fe",
   "metadata": {},
   "source": [
    "##### Info : Lat and Long kept has primary has it represents each spatial points in our study area (some geological core stuff to do in coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ========= EDIT THESE =========\n",
    "BASE_CSV = \"/Users/davidjerome/Desktop/Dissertation/Machine_learning/Data collection/FINAL_PDFS_DATA/Terrains_Cat_added_to_final.csv\"  # your final base file (will NOT be altered)\n",
    "\n",
    "# Each entry: (csv_path, {\"source_col_in_that_csv\": \"new_col_name_in_output\", ...})\n",
    "ADDONS = [\n",
    "    (\"distance_to_urban.csv\", {\n",
    "        \"dist_to_urban_m\": \"dist_to_urban_m\",\n",
    "    }),\n",
    "    (\"/Users/davidjerome/Desktop/Dissertation/Machine_learning/Data collection/FINAL_PDFS_DATA/Distance_to_roads_TableToExcel.csv\", {\n",
    "        \"distance_to_roads_m\": \"distance_to_roads_m\",\n",
    "    }),\n",
    "    (\"/Users/davidjerome/Desktop/Dissertation/Machine_learning/Data collection/FINAL_PDFS_DATA/points_human_proximity_vars_2024_per_point.csv\", {\n",
    "        \"dist_to_powerplant_m\": \"dist_to_powerplant_m\",\n",
    "        \"urban_flag\": \"urban_flag\",\n",
    "        \"viirs_avg_rad_2024\":\"viirs_avg_rad_2024\",\n",
    "        \"worldpop_100m\":\"worldpop_100m\",\n",
    "    }),\n",
    "]\n",
    "\n",
    "# If your add-on files use different headers for lat/lon, add them here (case-insensitive)\n",
    "LAT_CANDIDATES = [\"lat\", \"latitude\"]\n",
    "LON_CANDIDATES = [\"lon\", \"long\", \"longitude\"]\n",
    "# ==============================\n",
    "\n",
    "\n",
    "def read_txt(path):\n",
    "    # Read as text so nothing gets reformatted (dates stay dates-as-text, etc.)\n",
    "    return pd.read_csv(path, dtype=str, keep_default_na=False, na_filter=False)\n",
    "\n",
    "\n",
    "def find_col(df, candidates):\n",
    "    # return the actual column name matching any candidate (case/space tolerant)\n",
    "    normalized = {c.lower().replace(\" \", \"\").replace(\"_\", \"\"): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        k = c.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        if k in normalized:\n",
    "            return normalized[k]\n",
    "    return None\n",
    "\n",
    "\n",
    "def lookup_from_addon(path, mapping):\n",
    "    \"\"\"\n",
    "    path: CSV path\n",
    "    mapping: dict {source_col -> out_col}\n",
    "    returns a dataframe with columns: lat, lon, <out cols...>\n",
    "    \"\"\"\n",
    "    df = read_txt(path)\n",
    "\n",
    "    lat_col = find_col(df, LAT_CANDIDATES)\n",
    "    lon_col = find_col(df, LON_CANDIDATES)\n",
    "    if not lat_col or not lon_col:\n",
    "        raise ValueError(f\"{path}: couldn't find lat/lon columns. Saw: {list(df.columns)}\")\n",
    "\n",
    "    missing = [src for src in mapping.keys() if src not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path}: missing source columns {missing}. Saw: {list(df.columns)}\")\n",
    "\n",
    "    keep = [lat_col, lon_col] + list(mapping.keys())\n",
    "    tmp = df[keep].copy()\n",
    "\n",
    "    # collapse to one row per (lat,lon); take first non-empty for each value column\n",
    "    tmp = tmp.replace({\"\": None}).groupby([lat_col, lon_col], as_index=False).first()\n",
    "\n",
    "    # rename lat/lon to match base, and source cols to desired output names\n",
    "    rename_map = {lat_col: \"lat\", lon_col: \"lon\"}\n",
    "    rename_map.update(mapping)\n",
    "    return tmp.rename(columns=rename_map)\n",
    "\n",
    "\n",
    "# -------- main --------\n",
    "base = read_txt(BASE_CSV)\n",
    "if \"lat\" not in base.columns or \"lon\" not in base.columns:\n",
    "    raise ValueError(f\"Base must contain 'lat' and 'lon'. Saw: {list(base.columns)}\")\n",
    "\n",
    "base_cols = list(base.columns)          # preserve base order\n",
    "added_cols_in_order = []                # track new columns as we add them\n",
    "\n",
    "for path, mapping in ADDONS:\n",
    "    look = lookup_from_addon(path, mapping)\n",
    "    base = base.merge(look, on=[\"lat\", \"lon\"], how=\"left\")\n",
    "    added_cols_in_order.extend(mapping.values())\n",
    "\n",
    "# final column order: base columns first, then the new ones (in the order you listed)\n",
    "final_cols = base_cols + [c for c in added_cols_in_order if c in base.columns]\n",
    "base = base[final_cols]\n",
    "\n",
    "base.to_csv(\"final_dataset.csv\", index=False)\n",
    "print(\"✓ final_dataset.csv written (base unchanged; requested columns added).\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
